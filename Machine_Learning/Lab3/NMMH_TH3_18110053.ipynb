{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Khoa Học Tự Nhiên\n",
    "## Nhập Môn Máy Học - Lab3\n",
    "### Nguyễn Quốc Bảo - 18110053"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_DtPuBUEORG"
   },
   "source": [
    "# Logistic Regresion\n",
    "Ta định nghĩa với mọi $t \\in R$ thì hàm sigmoid được định nghĩa như sau:\n",
    "$$f(t)=\\frac{1}{1+\\epsilon^{-t}}$$\n",
    "$x_i \\in R^{n \\times 1}$ là một sample thứ $i$ trong tập dữ liệu,  $y_i \\in R^{1 \\times 1}$ là class của sample thứ $i$, $W \\in R^{n \\times 1}$ là trọng số mà ta cần tìm, ta có:\n",
    "\n",
    "$$\\hat{y_i}=f(x_i^TW)$$\n",
    "\n",
    "Ta định nghĩa hàm Loss như sau:\n",
    "\n",
    "$$Loss=\\frac{1}{N}\\sum^N-Ylog[f(X^TW)]-(1-Y)log[1-f(X^TW)]$$\n",
    "\n",
    "Với $X \\in R^{n \\times 1}$, $Y \\in R^{n \\times 1}$, $W \\in R^{n \\times 1}$.\n",
    "\n",
    "Đặt $Z=f(X^TW)$, ta có\n",
    "$$\\nabla_W Loss = -\\frac{1}{N} \\sum^N(\\frac{Y}{Z}-\\frac{1-Y}{1-Z})\\frac{\\partial Z}{\\partial W}$$\n",
    "\n",
    "Mà: $\\frac{\\partial Z}{\\partial W}=Z(1-Z)X$ nên:\n",
    "\n",
    "$$\\nabla_W Loss = -\\frac{1}{N} \\sum^N(Y-Z)X$$\n",
    "\n",
    "suy ra: \n",
    "$$W:=W-lr\\frac{1}{N} \\sum^N(Z-Y)X$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiP_kwLj9fXN"
   },
   "source": [
    "# Softmax Regression\n",
    "Ta hàm softmax như sau:\n",
    "$$a_i=\\frac{e^{X^Tw_i}}{\\sum_{j=1}^Ce^{X^Tw_i}}$$\n",
    "\n",
    "Với $w_i$ chính là trọng số cho hàm softmax của class thứ $i$. Nghĩa là với class thứ $i$ ta tương ứng có $a_i$ là hàm dự đoán xác xuất để sample $x_i$ rơi vào class này. $W=[w_1,w_2,...,w_C]$ là ma trận trọng số cần tìm, $W \\in R^{n \\times C}$, Với C là số Classes có trong dữ liệu\n",
    "\n",
    "Ngoài ta ta phải đổi y từ dạng scaler sang vector theo onehot encoding, tức là:\n",
    "$$y=[y_1,y_2,...,y_C]$$ \n",
    "với $\\sum_{i=1}^Cy_i=1$\n",
    "\n",
    "\n",
    "Ta định nghĩa hàm Loss như sau:\n",
    "\n",
    "$$Loss=\\frac{1}{N} \\sum^N(-\\sum_{i=1}^C y_i log(\\frac{e^{x^Tw_i}}{\\sum_j^Ce^{x^Tw_j}})) $$\n",
    "\n",
    "$$Loss=\\frac{1}{N} \\sum^N(-\\sum_{i=1}^C (y_ix^Tw_i-y_ilog(\\sum_j^Ce^{x^Tw_j}))) $$\n",
    "\n",
    "$$Loss=\\frac{1}{N} \\sum^N(-\\sum_{i=1}^C (y_ix^Tw_i) +log(\\sum_j^Ce^{x^Tw_j})) $$\n",
    "\n",
    "Gradient:\n",
    "$$\\nabla_W Loss = [\\frac{\\partial Loss}{\\partial w_1}, \\frac{\\partial Loss}{\\partial w_2},...,\\frac{\\partial Loss}{\\partial w_C}]$$\n",
    "\n",
    "Với $\\frac{\\partial Loss}{\\partial w_i}=\\frac{1}{N} \\sum^N(-y_i+\\frac{e^{x^Tw_i}}{\\sum_j^Ce^{x^Tw_j}})x$\n",
    "\n",
    "Từ đây ta có công thức cập nhật:\n",
    "\n",
    "$$W:= W- lr\\nabla_W Loss $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UpfabBedEKO8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5z4FCnqAmtE"
   },
   "source": [
    "# Bài tập"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8kP3tKp_IxN"
   },
   "source": [
    "1. Hãy xây dựng mô hình logistic regression bằng tất cả các features trong file heart, so sánh với thư viện sklearn.\n",
    "2. Hãy xây dựng mô hình softmax regression trên bộ Iris (nên Normalize data), so sánh với thư viện sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bài 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBoTD6c7upP8"
   },
   "source": [
    "- Data này là dùng các features như tuổi, giới tính, lượng cholesterol để dự đoán bệnh nhân có bị mắc bệnh tim mạch hay không.\n",
    "\n",
    "- target gồm 2 label 1 và 0 tương ứng là mắc bệnh hay không mắc bệnh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n",
      "(303, 13)\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(\"https://raw.githubusercontent.com/huynhthanh98/ML/master/lab-03/heart.csv\")\n",
    "print(data.shape)\n",
    "X = data.loc[:,:'thal'].values\n",
    "y = data['target'].values\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "min=np.min(X_train,axis=0)\n",
    "max=np.max(X_train,axis=0)\n",
    "X_train=(X_train-min)/(max -min)\n",
    "\n",
    "X_test=(X_test-min)/(max-min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train.reshape([-1,1])\n",
    "y_test=y_test.reshape([-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Thêm 1 cột những số 1 vào X_train\n",
    "X_train_bar=np.concatenate([np.ones([X_train.shape[0],1]),X_train],axis=1)\n",
    "X_train_bar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203, 13)\n",
      "(100, 13)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hàm sigmoid\n",
    "def sigmoid(W,X):\n",
    "  \"\"\"\n",
    "        X: array-like,\n",
    "            Input data sample\n",
    "        W: array-like,\n",
    "            Weight values\n",
    "  \"\"\"\n",
    "\n",
    "  return 1/(1+np.exp((-np.matmul(X,W))))\n",
    "\n",
    "#Hàm Normalization\n",
    "def Normalization(X_train,X_test):\n",
    "    \"\"\"\n",
    "        X_train: array-like \n",
    "                data train\n",
    "        X_test: array-like\n",
    "                data test\n",
    "    \"\"\"\n",
    "    min=np.min(X_train,axis=0)\n",
    "    max=np.max(X_train,axis=0)\n",
    "    X_train=(X_train-min)/(max -min)\n",
    "\n",
    "    X_test=(X_test-min)/(max-min)\n",
    "    return X_train,X_test\n",
    "\n",
    "# Hàm Concatenate value one for bias\n",
    "def Concatenate(X_train,X_test):\n",
    "    \"\"\"\n",
    "        X_train: array-like\n",
    "                data train.\n",
    "        X_test: array-like\n",
    "                data test    \n",
    "    \"\"\"\n",
    "    X_train_bar=np.concatenate([np.ones([X_train.shape[0],1]),X_train],axis=1)\n",
    "    X_test_bar=np.concatenate([np.ones([X_test.shape[0],1]),X_test],axis=1)\n",
    "    \n",
    "    return X_train_bar, X_test_bar\n",
    "\n",
    "# Model logistic regression\n",
    "def logistic_regression(X, y, W_init, lr = 0.001, max_iter = 50000):\n",
    "    \"\"\"\n",
    "        X:  array-like,\n",
    "            Input data.\n",
    "        y:  array-like,\n",
    "            Input label.\n",
    "        W_init: array-like\n",
    "            An initial weight.\n",
    "        lr:  float, default=0.001\n",
    "            learning rate\n",
    "        max_iter: default=10000\n",
    "            Maximum number of iterations taken for the solvers to converge.\n",
    "    \"\"\"\n",
    "\n",
    "    list_W=[W_init]\n",
    "    list_loss=[10]\n",
    "    for epoch in range(max_iter):\n",
    "        W=list_W[-1]\n",
    "        prediction=sigmoid(W,X) # Dự đoán\n",
    "        \n",
    "        #Tính loss function of logistic regression\n",
    "        loss=-np.mean((y*np.log(prediction)+(1-y)*np.log(1-prediction)),axis=0) \n",
    "        \n",
    "        #Tính Gradient\n",
    "        gradient=np.mean((prediction-y)*X ,axis=0) \n",
    "        gradient=gradient.reshape(-1,1)\n",
    "        \n",
    "        #cập nhật W\n",
    "        W=W-lr*gradient \n",
    "        \n",
    "        list_W.append(W)\n",
    "        list_loss.append(loss)\n",
    "        \n",
    "        if epoch % 1000==0:\n",
    "            print(\"Loss at iter {}: {}\".format(epoch, list_loss[-1]))\n",
    "            if (loss[-1] < 1e-5):\n",
    "                  return W, list_loss[-1] \n",
    "    return W, list_loss[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data train , test\n",
    "X_train, X_test = Normalization(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias:  [1.34612603]\n",
      "Matrix W: [[-0.04312529 -1.12273845  1.80008111 -0.2487577  -0.18460104  0.20025636\n",
      "   0.49782813  0.91506388 -1.10864927 -1.1569228   1.2377623  -2.54393322\n",
      "  -1.82254879]]\n",
      "Loss:  0.19\n",
      "score:  0.81\n"
     ]
    }
   ],
   "source": [
    "# Dùng thử với sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "print('Bias: ',clf.intercept_)\n",
    "print('Matrix W:',clf.coef_)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "MSE = mean_squared_error(y_test,y_pred)\n",
    "print('Loss: ', MSE)\n",
    "print('score: ',clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thêm 1 cột 1 vào data train, test\n",
    "X_train_bar, X_test_bar = Concatenate(X_train,X_test)\n",
    "\n",
    "y_train=y_train.reshape([-1,1])\n",
    "y_test=y_test.reshape([-1,1])\n",
    "\n",
    "# Khởi tạo W ban đầu\n",
    "W_init=np.random.randn(X_train_bar.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iter 0: [0.69394043]\n",
      "Loss at iter 1000: [0.53822487]\n",
      "Loss at iter 2000: [0.4980231]\n",
      "Loss at iter 3000: [0.48402815]\n",
      "Loss at iter 4000: [0.47570512]\n",
      "Loss at iter 5000: [0.46892759]\n",
      "Loss at iter 6000: [0.46284373]\n",
      "Loss at iter 7000: [0.4572439]\n",
      "Loss at iter 8000: [0.45204973]\n",
      "Loss at iter 9000: [0.4472137]\n",
      "Loss at iter 10000: [0.44269859]\n",
      "Loss at iter 11000: [0.4384727]\n",
      "Loss at iter 12000: [0.43450848]\n",
      "Loss at iter 13000: [0.43078169]\n",
      "Loss at iter 14000: [0.427271]\n",
      "Loss at iter 15000: [0.42395753]\n",
      "Loss at iter 16000: [0.42082451]\n",
      "Loss at iter 17000: [0.41785703]\n",
      "Loss at iter 18000: [0.41504176]\n",
      "Loss at iter 19000: [0.41236678]\n",
      "Loss at iter 20000: [0.40982139]\n",
      "Loss at iter 21000: [0.40739595]\n",
      "Loss at iter 22000: [0.40508178]\n",
      "Loss at iter 23000: [0.40287101]\n",
      "Loss at iter 24000: [0.40075652]\n",
      "Loss at iter 25000: [0.39873183]\n",
      "Loss at iter 26000: [0.39679105]\n",
      "Loss at iter 27000: [0.39492879]\n",
      "Loss at iter 28000: [0.39314014]\n",
      "Loss at iter 29000: [0.39142058]\n",
      "Loss at iter 30000: [0.38976599]\n",
      "Loss at iter 31000: [0.38817255]\n",
      "Loss at iter 32000: [0.38663677]\n",
      "Loss at iter 33000: [0.3851554]\n",
      "Loss at iter 34000: [0.38372547]\n",
      "Loss at iter 35000: [0.3823442]\n",
      "Loss at iter 36000: [0.38100903]\n",
      "Loss at iter 37000: [0.37971758]\n",
      "Loss at iter 38000: [0.37846762]\n",
      "Loss at iter 39000: [0.37725711]\n",
      "Loss at iter 40000: [0.37608411]\n",
      "Loss at iter 41000: [0.37494682]\n",
      "Loss at iter 42000: [0.37384358]\n",
      "Loss at iter 43000: [0.37277281]\n",
      "Loss at iter 44000: [0.37173303]\n",
      "Loss at iter 45000: [0.37072286]\n",
      "Loss at iter 46000: [0.36974101]\n",
      "Loss at iter 47000: [0.36878626]\n",
      "Loss at iter 48000: [0.36785745]\n",
      "Loss at iter 49000: [0.36695351]\n",
      "final W:  [[-0.54376087]\n",
      " [ 1.24157679]\n",
      " [-1.06012789]\n",
      " [ 1.83118109]\n",
      " [ 0.31410761]\n",
      " [-0.26318313]\n",
      " [ 0.01776994]\n",
      " [-0.1482837 ]\n",
      " [ 1.65175825]\n",
      " [-1.1654623 ]\n",
      " [-2.28357778]\n",
      " [ 1.07814381]\n",
      " [-2.29563759]\n",
      " [-0.50484532]]\n",
      "final loss:  [0.36607428]\n"
     ]
    }
   ],
   "source": [
    "W, loss = logistic_regression(X_train_bar, y_train, W_init, lr = 0.0011)\n",
    "print('final W: ',W)\n",
    "print('final loss: ',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "predict = sigmoid(W,X_test_bar)\n",
    "y_pred = np.where(predict > 0.5, 1, 0)\n",
    "np.mean(y_pred==y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bài 2 \n",
    "#### Hãy xây dựng mô hình softmax regression trên bộ Iris (nên Normalize data), so sánh với thư viện sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data train (100, 4)\n",
      "Shape of data test (100, 4)\n",
      "\n",
      "Shape of X_train_bar (100, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X, y, test_size=0.33, random_state=42)\n",
    "print('Shape of data train',X_train.shape)\n",
    "print('Shape of data test',X_train.shape)\n",
    "print()\n",
    "X_train, X_test = Normalization(X_train, X_test)\n",
    "\n",
    "\n",
    "X_train_bar, X_test_bar = Concatenate(X_train,X_test)\n",
    "print('Shape of X_train_bar',X_train_bar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOgqfNVqu_yc"
   },
   "source": [
    "- Data này gồm 2 features là x1 và x2\n",
    "- label gồm 2 class là 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n"
     ]
    }
   ],
   "source": [
    "# Hàm mã hóa label \n",
    "def Onehot_encoder(y_train, y_test):\n",
    "    \"\"\"\n",
    "        y_train: array-like,\n",
    "                label train\n",
    "        y_test: array-like,\n",
    "                label test\n",
    "    \"\"\"\n",
    "    y_train_onehot = np.zeros( (y_train.size, y_train.max() + 1),dtype=int)\n",
    "    y_train_onehot[np.arange(y_train.size), y_train.reshape(-1)] = 1\n",
    "\n",
    "    y_test_onehot = np.zeros( (y_test.size, y_test.max() + 1) ,dtype=int)\n",
    "    y_test_onehot[np.arange(y_test.size), y_test.reshape(-1)] = 1\n",
    "    return y_train_onehot, y_test_onehot\n",
    "\n",
    "y_train_onehot, y_test_onehot = Onehot_encoder(y_train, y_test)\n",
    "print(y_train_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias:  [ 1.75179038  1.13814666 -2.88993704]\n",
      "Matrix W: [[-1.1990263   1.32049321 -2.43592642 -2.25540126]\n",
      " [ 0.11103058 -1.40517501  0.39820193 -0.31581872]\n",
      " [ 1.08799572  0.0846818   2.03772449  2.57121997]]\n",
      "Loss:  0.1\n",
      "score:  0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "print('Bias: ',clf.intercept_)\n",
    "print('Matrix W:',clf.coef_)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "MSE = mean_squared_error(y_test,y_pred)\n",
    "print('Loss: ', MSE)\n",
    "print('score: ',clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, W):\n",
    "    \"\"\"\n",
    "        X: array-like,\n",
    "            Input data\n",
    "        W: array-like,\n",
    "            Weight values\n",
    "    \"\"\"\n",
    "    expi = np.exp(np.matmul(X,W)) \n",
    "    exp_sum = np.sum(expi,axis=1)    \n",
    "    soft = expi/exp_sum.reshape(-1,1)\n",
    "    return soft\n",
    "\n",
    "\n",
    "def softmax_model(X, y, W_init, lr = 0.001, max_iter = 200000):\n",
    "    \"\"\"\n",
    "        X:  array-like,\n",
    "            Input data.\n",
    "        y:  array-like,\n",
    "            Input label.\n",
    "        W_init: array-like\n",
    "            An initial weight.\n",
    "        lr:  float, default=0.001\n",
    "            learning rate\n",
    "        max_iter: default=20000\n",
    "            Maximum number of iterations taken for the solvers to converge.\n",
    "    \"\"\"\n",
    "    list_W=[W_init]\n",
    "    list_loss=[0]\n",
    "    for epoch in range(max_iter):\n",
    "        soft1 = softmax(X, list_W[-1])\n",
    "        a = (- y + soft1)\n",
    "        W_temp = np.zeros([X.shape[1],y.shape[1]])\n",
    "        #W_temp\n",
    "        for i in range(y.shape[1]):\n",
    "            temp=i+1\n",
    "            W_temp[:,i:temp]=np.mean(a[:,i:temp]*X ,axis=0).reshape(-1,1)\n",
    "    #cal error\n",
    "        W=list_W[-1]-lr*W_temp\n",
    "\n",
    "        for i in range(y_train_onehot.shape[1]):\n",
    "            loss = softmax(X,W)\n",
    "            loss = -y*np.log(loss)\n",
    "            loss = np.sum(np.mean(loss,axis=0))\n",
    "\n",
    "\n",
    "        list_W.append(W)\n",
    "        list_loss.append(loss)\n",
    "\n",
    "        if epoch % 10000==0:\n",
    "            print(list_loss[-1])\n",
    "    return list_W[-1],list_loss[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8851002177876963\n",
      "0.4927886309811343\n",
      "0.37885448386147585\n",
      "0.32295285789328887\n",
      "0.2872082485332189\n",
      "0.26137929243085967\n",
      "0.24140822116787045\n",
      "0.22530549370665606\n",
      "0.21194845417739194\n",
      "0.20063856381479833\n",
      "0.1909097270704635\n",
      "0.18243462401400332\n"
     ]
    }
   ],
   "source": [
    "W_init = np.random.randn(X_train_bar.shape[1],len(np.unique(y)))\n",
    "W, loss = softmax_model(X_train_bar, y_train_onehot, W_init, lr = 0.005)\n",
    "print('final W: ',W)\n",
    "print('final loss: ',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "predict = softmax(X_test_bar,W) \n",
    "#Kết quả\n",
    "np.mean(y_test.reshape(-1)==np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of lab_03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
